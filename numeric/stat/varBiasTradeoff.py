import numpy as np
from sklearn.model_selection import train_test_split

# plotting 
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.animation import FuncAnimation, PillowWriter 

# load default theme for Seaborn
sns.set()

# set random seed for reproducibility
np.random.seed(42)

def f(X):
    y = -0.02 * X ** 4 + 0.1 * X ** 3 + 0.6 * X ** 2 - 1.2 * X + 2 * np.sin(X) + np.cos(X)
    string_representation = r"-0.02x^{4} + 0.1x^{3} + 0.6x^{2} - 1.2x + 2\sin(x) + \cos(x)"
    return y, string_representation

def generate_data(input_predictors):
    white_noise = np.random.normal(size=input_predictors.shape)
    real_values, string_representation = f(input_predictors)
    # generate a title for the plot that will describe the way data is generated
    string_representation = fr"$y = {string_representation} + \varepsilon, \varepsilon \sim \mathcal{{N}}(0, 1)$"
    observed_outcomes = real_values + white_noise
    return real_values, observed_outcomes, string_representation

def plot_generated_data(data, string_representation, test_data=None, true_data=None):
    figure, axes = plt.subplots(1, 1, figsize=(10, 8), dpi=200)
    # set general plot parameters
    axes.set_title(string_representation)
    axes.set_xlabel("Input predictors")
    axes.set_ylabel("Observed outcome")
    axes.grid(visible=True)
    # plot the data and the legend
    axes.scatter(data[0], data[1], marker=".", color="b", label="Train data")
    if test_data is not None:
        axes.scatter(test_data[0], test_data[1], marker=".", color="r", label="Test data")
    if true_data is not None:
        axes.plot(true_data[0], true_data[1], color="g", linewidth=1.5, label="Underlying function")
    axes.legend()
    # show the image and close the current figure
    plt.show()


def split_data(X, y, f, test_split_size=0.2):
    X_train, X_test, y_train, y_test, f_train, f_test = train_test_split(X, y, f, test_size=test_split_size)
    return (X_train, y_train, f_train), (X_test, y_test, f_test)

# generate 1000 points with noise that will become our dataset
input_predictors = np.linspace(-4, 4, 1000)
real_values, observed_outcomes, string_representation = generate_data(input_predictors)

# split them into train and test sets, the test set will contain 20% of all generated points
data_train, data_test = split_data(input_predictors, observed_outcomes, real_values)

# plot the data with the underlying function
plot_generated_data(data_train, string_representation, data_test, true_data=(input_predictors, real_values))


max_polynomial_degree, num_bootstraps = 10, 1000

biases, variances, losses = [], [], []

for polynomial_degree in range(max_polynomial_degree):
    y_test_bootstrap_preds = []
    for _ in range(num_bootstraps):
        # get a random subset of train dataset
        bootstrap_idx = np.random.choice(len(data_train[0]), len(data_train[0]) // 5)
        # fit a polynomial regression model on said subset
        fit_coeff = np.polyfit(data_train[0][bootstrap_idx], data_train[1][bootstrap_idx], deg=polynomial_degree + 1)
        # collect predictions for test set generated by said model
        y_test_bootstrap_preds.append(np.polyval(fit_coeff, data_test[0]))
    # compute mean prediction over bootstraps
    mean_y_test_pred = np.mean(y_test_bootstrap_preds, axis=0)
    
    biases.append(((mean_y_test_pred - data_test[2]) ** 2).mean())
    variances.append(((y_test_bootstrap_preds - mean_y_test_pred) ** 2).mean())
    losses.append(((y_test_bootstrap_preds - data_test[1]) ** 
                   2).mean(axis=1).mean())

best_polynomial_degree = np.argmin(losses)

fig, ax = plt.subplots(dpi=200)
ax.plot(np.arange(1, max_polynomial_degree + 1), biases, color="g", zorder=1)
ax.plot(np.arange(1, max_polynomial_degree + 1), losses, color="k", linestyle="--", label="Loss", zorder=1)
ax.set_ylabel("Bias", color="g")
ax.grid(visible=True, zorder=0)

ax2 = ax.twinx()
ax2.plot(np.arange(1, max_polynomial_degree + 1), variances, color="b", zorder=1)
ax2.set_ylabel("Variance", color="b")
ax2.grid(visible=False)

ax.scatter([best_polynomial_degree + 1], [losses[best_polynomial_degree]], color="r",  label=fr"Best model ($n = {best_polynomial_degree + 1}$)")
ax.set_xlabel("Polynomial Degree")
ax.set_title("Bias-variance tradeoff")
ax.legend(loc="upper center")
plt.show()