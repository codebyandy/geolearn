{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading package hydroDL\n"
     ]
    }
   ],
   "source": [
    "from hydroDL import kPath # package by Kuai Fang, kPath contains req paths\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module by Kuai Fang\n",
    "from hydroDL.data import dbVeg\n",
    "from hydroDL.data import DataModel\n",
    "from hydroDL.master import dataTs2Range\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# satellite variable names\n",
    "varS = ['VV', 'VH', 'vh_vv']\n",
    "varL = ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'ndvi', 'ndwi', 'nirv']\n",
    "varM = [\"MCD43A4_b{}\".format(x) for x in range(1, 8)]\n",
    "\n",
    "# Days per satellite\n",
    "bS = 8\n",
    "bL = 6\n",
    "bM = 10\n",
    "\n",
    "def prepare_data(dataName, rho):\n",
    "    \"\"\"\n",
    "    Loads the data, normalizes it, and processes it into the required shape and format \n",
    "    for further analysis. It identifies the available days for each observation from various remote \n",
    "    sensing sources (Sentinel, Landsat, MODIS) and filters the observations to retain only those with \n",
    "    data available from all sources.\n",
    "\n",
    "    Args:\n",
    "        dataName (str): The name or path of the dataset to be loaded.\n",
    "        rho (int): The time window size for each observation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the following elements:\n",
    "            - df (DataFrameVeg): A custom DataFrameVeg object containing the loaded data.\n",
    "            - dm (DataModel): A custom DataModel object containing the normalized data.\n",
    "            - iInd (np.array): Array of day indices for the observations.\n",
    "            - jInd (np.array): Array of site indices for the observations.\n",
    "            - nMat (np.array): Matrix indicating the number of available days with data for each satellite \n",
    "                               (Sentinel, Landsat, MODIS) for each observation.\n",
    "            - pSLst (list): List of arrays indicating the days with available data for Sentinel for each observation.\n",
    "            - pLLst (list): List of arrays indicating the days with available data for Landsat for each observation.\n",
    "            - pMLst (list): List of arrays indicating the days with available data for MODIS for each observation.\n",
    "            - x (np.array): Array of raw values for each observation in the shape (rho, number of observations, number of input features).\n",
    "            - rho (int): The time window size for each observation.\n",
    "            - xc (np.array): Array of constant variables for the observations.\n",
    "            - yc (np.array): Array of LFMC data for the observations.\n",
    "    \"\"\"\n",
    "    # Load data with custom DataFrameVeg and DataModel classes\n",
    "    df = dbVeg.DataFrameVeg(dataName)\n",
    "    dm = DataModel(X=df.x, XC=df.xc, Y=df.y)\n",
    "    dm.trans(mtdDefault='minmax') # Min-max normalization\n",
    "    dataTup = dm.getData()\n",
    "\n",
    "    # To convert data to shape (Number of observations, rho, number of input features)\n",
    "    dataEnd, (iInd, jInd) = dataTs2Range(dataTup, rho, returnInd=True) # iInd: day, jInd: site\n",
    "    x, xc, _, yc = dataEnd \n",
    "   \n",
    "    iInd = np.array(iInd) # TODO: Temporary fix\n",
    "    jInd = np.array(jInd) # TODO: emporary fix\n",
    "    \n",
    "    iS = [df.varX.index(var) for var in varS]\n",
    "    iM = [df.varX.index(var) for var in varM]\n",
    "    \n",
    "    # For each remote sensing source (i.e. Sentinel, MODIS), for each LFMC observaton,\n",
    "    # create a list of days in the rho-window that have data \n",
    "    # nMat: Number of days each satellite has data for, of shape (# obsevations, # satellites)\n",
    "    pSLst, pMLst = list(), list()\n",
    "    nMat = np.zeros([yc.shape[0], 2])\n",
    "    for k in range(nMat.shape[0]):\n",
    "        tempS = x[:, k, iS]\n",
    "        pS = np.where(~np.isnan(tempS).any(axis=1))[0]\n",
    "        pSLst.append(pS)\n",
    "        tempM = x[:, k, iM]\n",
    "        pM = np.where(~np.isnan(tempM).any(axis=1))[0]\n",
    "        pMLst.append(pM)\n",
    "        nMat[k, :] = [len(pS), len(pM)]\n",
    "    \n",
    "    # only keep if data if there is at least 1 day of data for each remote sensing source\n",
    "    indKeep = np.where((nMat > 0).all(axis=1))[0]\n",
    "    x = x[:, indKeep, :]\n",
    "    xc = xc[indKeep, :]\n",
    "    yc = yc[indKeep, :]\n",
    "    nMat = nMat[indKeep, :]\n",
    "    pSLst = [pSLst[k] for k in indKeep]\n",
    "    pMLst = [pMLst[k] for k in indKeep]\n",
    "    \n",
    "    jInd = jInd[indKeep]\n",
    "    iInd = iInd[indKeep]\n",
    "\n",
    "    data = {\n",
    "        'dataFrame' : df,\n",
    "        'dataModel' : dm,\n",
    "        'day_indices' : iInd,\n",
    "        'site_indices' : jInd,\n",
    "        'sat_avail_per_obs' : nMat, \n",
    "        's_days_per_obs' : pSLst,\n",
    "        'm_days_per_obs': pMLst,\n",
    "        'rho' : rho,\n",
    "        'x' : x,\n",
    "        'xc' : xc,\n",
    "        'yc' : yc\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_total_sites(splits_dict):\n",
    "    total_sites = []\n",
    "    for fold in range(5):\n",
    "        num_sites = len(splits_dict[f'trainSite_k{fold}5']) + len(splits_dict[f'testSite_k{fold}5'])\n",
    "        total_sites.append(num_sites)\n",
    "        \n",
    "    assert all(x == total_sites[0] for x in total_sites)\n",
    "\n",
    "def test_obs_duplicates(splits_dict):\n",
    "    for fold in range(5):\n",
    "        train_ind = set(splits_dict[f'trainInd_k{fold}5'])\n",
    "        test_qual_ind = set(splits_dict[f'testInd_k{fold}5'])\n",
    "        test_poor_ind = set(splits_dict[f'testInd_underThresh'])\n",
    "\n",
    "        assert len(train_ind.intersection(test_qual_ind)) == 0\n",
    "        assert len(train_ind.intersection(test_poor_ind)) == 0\n",
    "        assert len(test_qual_ind.intersection(test_poor_ind)) == 0\n",
    "\n",
    "def test_site_duplicates(splits_dict):\n",
    "    for fold in range(5):\n",
    "        train_sites = set(splits_dict[f'trainSite_k{fold}5'])\n",
    "        test_qual_sites = set(splits_dict[f'testSite_k{fold}5'])\n",
    "        test_poor_sites = set(splits_dict[f'testSite_underThresh'])\n",
    "\n",
    "        assert len(train_sites.intersection(test_qual_sites)) == 0\n",
    "        assert len(train_sites.intersection(test_poor_sites)) == 0\n",
    "        assert len(test_qual_sites.intersection(test_poor_sites)) == 0\n",
    "\n",
    "def test_site_from_obs_duplicates(splits_dict, jInd):\n",
    "    for fold in range(5):\n",
    "        train_obs = splits_dict[f'trainInd_k{fold}5']\n",
    "        test_qual_obs = splits_dict[f'testInd_k{fold}5']\n",
    "        test_poor_obs = splits_dict[f'testInd_underThresh']\n",
    "\n",
    "        train_sites = set(jInd[train_obs])\n",
    "        test_qual_sites = set(jInd[test_qual_obs])\n",
    "        test_poor_sites = set(jInd[test_poor_obs])\n",
    "\n",
    "        assert len(train_sites.intersection(test_qual_sites)) == 0\n",
    "        assert len(train_sites.intersection(test_poor_sites)) == 0\n",
    "        assert len(test_qual_sites.intersection(test_poor_sites)) == 0\n",
    "\n",
    "def sanity_check_splits(dataset, rho, split_version):\n",
    "    data = prepare_data(dataset, rho)\n",
    "    jInd = data['site_indices']\n",
    "\n",
    "    splits_path = os.path.join(kPath.dirVeg, 'model', 'attention', split_version, 'subset.json')\n",
    "    with open(splits_path) as f:\n",
    "        splits_dict = json.load(f)\n",
    "\n",
    "    test_total_sites(splits_dict)\n",
    "    test_obs_duplicates(splits_dict)\n",
    "    test_site_duplicates(splits_dict)\n",
    "    test_site_from_obs_duplicates(splits_dict, jInd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'singleDaily-modisgrid-new-const'\n",
    "rho = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m split_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msanity_check_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_version\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 54\u001b[0m, in \u001b[0;36msanity_check_splits\u001b[0;34m(dataset, rho, split_version)\u001b[0m\n\u001b[1;32m     52\u001b[0m test_obs_duplicates(splits_dict)\n\u001b[1;32m     53\u001b[0m test_site_duplicates(splits_dict)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mtest_site_from_obs_duplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplits_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjInd\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 39\u001b[0m, in \u001b[0;36mtest_site_from_obs_duplicates\u001b[0;34m(splits_dict, jInd)\u001b[0m\n\u001b[1;32m     36\u001b[0m test_qual_sites \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(jInd[test_qual_obs])\n\u001b[1;32m     37\u001b[0m test_poor_sites \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(jInd[test_poor_obs])\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_sites\u001b[38;5;241m.\u001b[39mintersection(test_qual_sites)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_sites\u001b[38;5;241m.\u001b[39mintersection(test_poor_sites)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(test_qual_sites\u001b[38;5;241m.\u001b[39mintersection(test_poor_sites)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "split_version = 'dataset'\n",
    "sanity_check_splits(dataset, rho, split_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_version = 'stratified'\n",
    "sanity_check_splits(dataset, rho, split_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_version = 'stratified_s0'\n",
    "sanity_check_splits(dataset, rho, split_version)\n",
    "\n",
    "split_version = 'stratified_s1'\n",
    "sanity_check_splits(dataset, rho, split_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_version = 'random_s1'\n",
    "sanity_check_splits(dataset, rho, split_version)\n",
    "\n",
    "split_version = 'random_s1'\n",
    "sanity_check_splits(dataset, rho, split_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
